{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 11: Unsupervised Learning Assignment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Principal Component Analysis (PCA)\n",
    "In this section, you will work with the PCA algorithm in order to understand its definition and explore its uses."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Principle of Maximum Variance: what is PCA supposed to do?\n",
    "First of all, let us recall the principle/assumption of PCA:\n",
    "1. What is the variance?\n",
    "3. What is the covariance?\n",
    "3. How do we compute covariance?\n",
    "2. What is the meaning of the principle of maximum variance?\n",
    "4. Why do we need this principle?\n",
    "5. Does the principle always apply?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answers:** enter your answers here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation: how is PCA implemented?\n",
    "Here we implement the basic steps of PCA and assemble them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing libraries\n",
    "We start by importing the *numpy* library for performing matrix computations, the *pyplot* library for plotting data, and the *syntheticdata* module to import synthetic data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import syntheticdata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Centering the Data\n",
    "Implement a function with the following signature to center the data as explained in *Marsland*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def center_data(A):\n",
    "    # INPUT:\n",
    "    # A    [NxM] numpy data matrix (N samples, M features)\n",
    "    #\n",
    "    # OUTPUT:\n",
    "    # X    [NxM] numpy centered data matrix (N samples, M features)\n",
    "    \n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test your function, checking the following assertion on *testcase*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testcase = np.array([[3.,11.,4.3],[4.,5.,4.3],[5.,17.,4.5]])\n",
    "answer = np.array([[-1.,0.,4.3-(13.1/3.)],[0,-6.,4.3-(13.1/3.)],[1,6.,4.5-(13.1/3.)]])\n",
    "assert(np.all(center_data(testcase)==answer))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computing Covariance Matrix\n",
    "Implement a function with the following signature to compute the covariance matrix as explained in *Marsland*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_covariance_matrix(A):\n",
    "    # INPUT:\n",
    "    # A    [NxM] numpy data matrix (N samples, M features)\n",
    "    #\n",
    "    # OUTPUT:\n",
    "    # C    [NxM] numpy covariance matrix (N samples, M features)\n",
    "    \n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test your function, checking the following assertion on *testcase*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testcase = np.array([[22.,11.,5.5],[10.,5.,2.5],[34.,17.,8.5]])\n",
    "answer = np.array([[580.,290.,145.],[290.,145.,72.5],[145.,72.5,36.25]])\n",
    "assert(np.all(compute_covariance_matrix(testcase)==answer))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computing eigenvalues and eigenvectors\n",
    "Use the linear algebra package of *numpy* and its function *linalg.eig()* to compute eigenvalues and eigenvectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_eigenvalue_eigenvectors(A):\n",
    "    # INPUT:\n",
    "    # A    [DxD] numpy matrix\n",
    "    #\n",
    "    # OUTPUT:\n",
    "    # eigval    [D] numpy vector of eigenvalues\n",
    "    # eigvec    [DxD] numpy array of eigenvectors\n",
    "    \n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test your function, checking the following assertion on *testcase*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testcase = np.array([[2,0,0],[0,5,0],[0,0,3]])\n",
    "answer1 = np.array([2.,5.,3.])\n",
    "answer2 = np.array([[1.,0.,0.],[0.,1.,0.],[0.,0.,1.]])\n",
    "x,y = compute_eigenvalue_eigenvectors(testcase)\n",
    "assert(np.all(x==answer1))\n",
    "assert(np.all(y==answer2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sorting eigenvalues and eigenvectors\n",
    "Implement a function with the following signature to sort eigenvalues and eigenvectors as explained in *Marsland*.\n",
    "\n",
    "Remember that eigenvalue *eigval[i]* corresponds to eigenvector *eigvec[:,i]*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sort_eigenvalue_eigenvectors(eigval, eigvec):\n",
    "    # INPUT:\n",
    "    # eigval    [D] numpy vector of eigenvalues\n",
    "    # eigvec    [DxD] numpy array of eigenvectors\n",
    "    #\n",
    "    # OUTPUT:\n",
    "    # sorted_eigval    [D] numpy vector of eigenvalues\n",
    "    # sorted_eigvec    [DxD] numpy array of eigenvectors\n",
    "    \n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test your function, checking the following assertion on *testcase*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testcase = np.array([[2,0,0],[0,5,0],[0,0,3]])\n",
    "answer1 = np.array([5.,3.,2.])\n",
    "answer2 = np.array([[0.,0.,1.],[1.,0.,0.],[0.,1.,0.]])\n",
    "x,y = compute_eigenvalue_eigenvectors(testcase)\n",
    "x,y = sort_eigenvalue_eigenvectors(x,y)\n",
    "assert(np.all(x==answer1))\n",
    "assert(np.all(y==answer2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA Algorithm\n",
    "Implement a function with the following signature to compute PCA as explained in *Marsland*, using the functions implemented above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PCA(A,m):\n",
    "    # INPUT:\n",
    "    # A    [NxM] numpy data matrix (N samples, M features)\n",
    "    # m    integer number denoting the number of learned features (m <= M)\n",
    "    #\n",
    "    # OUTPUT:\n",
    "    # pca_eigvec    [mxM] numpy matrix containing the eigenvectors (m eigenvectors, M dimensions)\n",
    "    # P    [Nxm] numpy PCA data matrix (N samples, m features)\n",
    "    \n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test your function, checking the following assertion on *testcase*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testcase = np.array([[22.,11.,5.5],[10.,5.,2.5],[34.,17.,8.5]])\n",
    "x,y = PCA(testcase,2)\n",
    "import pickle\n",
    "answer1_file = open('PCAanswer1.pkl','rb'); answer2_file = open('PCAanswer2.pkl','rb')\n",
    "answer1 = pickle.load(answer1_file); answer2 = pickle.load(answer2_file)\n",
    "assert(np.all(x==answer1))\n",
    "assert(np.all(y==answer2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding: how does PCA work?\n",
    "We now use the PCA algorithm you implemented on a toy data set in order to understand its inner workings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the data\n",
    "The module *syntheticdata* provides a small synthetic dataset of dimension [100x2] (100 samples, 2 features)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = syntheticdata.get_synthetic_data1()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing the data\n",
    "Visualize the synthetic data using the function *scatter()* from the *matplotlib* library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X[:,0],X[:,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize the centered data\n",
    "Notice that the data visualized above is not centered on the origin (0,0). Use the function defined above to center the data, and then replot it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize the first eigenvector\n",
    "Visualize the vector defined by the first eigenvector.\n",
    "To do this you need to:\n",
    "- Use the *PCA()* function to recover the eigenvectors\n",
    "- Plot the centered data as done above \n",
    "- The first eigenvector is a 2D vector (x0,y0). This defines a vector with origin in (0,0) and head in (x0,y0). Use the function *plot()* from matplotlib to plot a line over the first eigenvector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_eigvec, _ = None\n",
    "first_eigvec = None\n",
    "\n",
    "plt.scatter(None,None)\n",
    "\n",
    "x = np.linspace(-5, 5, 1000)\n",
    "y = first_eigvec[1]/first_eigvec[0] * x\n",
    "plt.plot(x,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize the PCA projection\n",
    "Finally, use the *PCA()* algorithm to project on a single dimension and visualize the result using again the *scatter()* function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_,P = None\n",
    "plt.scatter(None,np.ones(X.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation: when are the results of PCA sensible?\n",
    "So far we have used PCA on synthetic data. Let us now imagine we are using PCA as a pre-processing step before a classification task. This is a common setup with high-dimensional data. We explore when the use of PCA is sensible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the first set of labels\n",
    "The function *get_synthetic_data_with_labels1()* from the module *syntethicdata* provides a first labeled dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X,y = syntheticdata.get_synthetic_data_with_labels1()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running PCA\n",
    "Process the data using the PCA algorithm and project it in one dimension. Plot the labeled data using *scatter()* before and after running PCA. Comment on the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(None,None,c=y[:,0])\n",
    "\n",
    "plt.figure()\n",
    "_,P = None\n",
    "plt.scatter(None,np.ones(X.shape[0]),c=y[:,0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Comment:** enter your comment here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the second set of labels\n",
    "The function *get_synthetic_data_with_labels2()* from the module *syntethicdata* provides a second labeled dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X,y = syntheticdata.get_synthetic_data_with_labels2()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running PCA\n",
    "As before, process the data using the PCA algorithm and project it in one dimension. Plot the labeled data using *scatter()* before and after running PCA. Comment on the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Comment:** enter your comment here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How would the result change if you were to consider the second eigenvector? Or if you were to consider both eigenvectors?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:** enter your answer here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Case study 1: PCA for visualization\n",
    "We now consider the *iris* dataset, a simple collection of data (N=150) describing iris flowers with four (M=4) features. The features are: Sepal Length, Sepal Width, Petal Length and Petal Width. Each sample has a label, identifying each flower as one of 3 possible types of iris: Setosa, Versicolour, and Virginica.\n",
    "\n",
    "Visualizing a 4-dimensional dataset is impossible; therefore we will use PCA to project our data in 2 dimensions and visualize it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the data\n",
    "The function *get_iris_data()* from the module *syntethicdata* returns the *iris* dataset. It returns a data matrix of dimension [150x4] and a label vector of dimension [150]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X,y = syntheticdata.get_iris_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing the data by selecting features\n",
    "Try to visualize the data (using label information) by randomly selecting two out of the four features of the data. You may try different pairs of features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing the data by PCA\n",
    "Process the data using PCA and visualize it (using label information). Compare with the previous visualization and comment on the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Comment:** enter your comment here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Case study 2: PCA for compression\n",
    "We now consider the *faces in the wild (lfw)* dataset, a collection of pictures (N=1280) of people. Each pixel in the image is a feature (M=2914)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the data\n",
    "The function *get_lfw_data()* from the module *syntethicdata* returns the *lfw* dataset. It returns a data matrix of dimension [1280x2914] and a label vector of dimension [1280]. It also returns two parameters, $h$ and $w$, reporting the height and the width of the images (these parameters are necessary to plot the data samples as images)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X,y,h,w = syntheticdata.get_lfw_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspecting the data\n",
    "Choose one datapoint to visualize (first coordinate of the matrix $X$) and use the function [imshow()](https://matplotlib.org/3.2.1/api/_as_gen/matplotlib.pyplot.imshow.html) to plot and inspect some of the pictures.\n",
    "\n",
    "Notice that *imshow* receives as a first argument an image to be plot; the image must be provided as a rectangular matrix, therefore we reshape a sample from the matrix $X$ to have height $h$ and width $w$. The parameter *cmap* specifies the color coding; in our case we will visualize the image in black-and-white with different gradations of grey."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(X[0,:].reshape((h, w)), cmap=plt.cm.gray)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementing a compression-decompression function\n",
    "Implement a function that first uses PCA to project samples in low-dimensions, and then reconstruct the original image.\n",
    "\n",
    "*Hint:* Most of the code is the same as the previous PCA() function you implemented. You may want to refer to *Marsland* to check out how reconstruction is performed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_decode_PCA(A,m):\n",
    "    # INPUT:\n",
    "    # A    [NxM] numpy data matrix (N samples, M features)\n",
    "    # m    integer number denoting the number of learned features (m <= M)\n",
    "    #\n",
    "    # OUTPUT:\n",
    "    # Ahat [NxM] numpy PCA reconstructed data matrix (N samples, M features)\n",
    "    \n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compressing and decompressing the data\n",
    "Use the implemented function to encode and decode the data by projecting on a lower dimensional space of dimension 200 (m=200)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xhat = encode_decode_PCA(X,None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspecting the reconstructed data\n",
    "Use the function *imshow* (as done before) to visualize and compare original and reconstructed pictures. Comment on the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Comment:** enter your comment here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating different compressions\n",
    "Use the previous setup to generate compressed images using different values of dimensions in the PCA algorithm (e.g.: 100, 200, 500, 1000). Plot and comment on the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(X[0,:].reshape((h, w)), cmap=plt.cm.gray)\n",
    "\n",
    "plt.figure()\n",
    "Xhat = encode_decode_PCA(X,None)\n",
    "plt.imshow(Xhat[0,:].reshape((h, w)), cmap=plt.cm.gray)\n",
    "\n",
    "None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Comment:** enter your comment here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-Means Clustering\n",
    "In this section you will use the *k-means clustering* algorithm to perform unsupervised clustering."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing scikit-learn library\n",
    "We start by importing the module *cluster.KMeans* from the standard machine learning library *scikit-learn*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the data\n",
    "We will use once again the *iris* data set. The function *get_iris_data()* from the module *syntethicdata* returns the *iris* dataset. It returns a data matrix of dimension [150x4] and a label vector of dimension [150]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X,y = syntheticdata.get_iris_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Projecting the data using PCA\n",
    "To allow for visualization, we project our data in two dimensions as we did previously. This step is not necessary, and we may want to try to use *k-means* later without the PCA pre-processing. However, we use PCA, as this will allow for an easy visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_,P = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running k-means\n",
    "Use the class *KMeans* to fit and predict the output of the *k-means* algorithm on the projected data. Run the algorithm using the following values of $k=\\{2,3,4,5\\}$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "KM = KMeans(None)\n",
    "yhat2 = KM.fit_predict(P)\n",
    "\n",
    "None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Qualitative assessment\n",
    "Plot the results of running the k-means algorithm, compare with the true labels, and comment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(P[:,0],P[:,1],c=None)\n",
    "plt.title('k=2')\n",
    "\n",
    "None\n",
    "\n",
    "plt.figure()\n",
    "plt.scatter(P[:,0],P[:,1],c=y)\n",
    "plt.title('Original data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Comment:** enter your comment here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optional 1: PCA Tuning\n",
    "If we use PCA for compression or decompression, it may not be trivial to decide how many dimensions to keep. In this section we review a principled way to decide how many dimensions to keep.\n",
    "\n",
    "The number of dimensions to keep is the only *hyper-parameter* of PCA. A method designed to decide how many dimensions/eigenvectors is the *proportion of variance*:\n",
    "$$ \\textrm{POV}=\\frac{\\sum_{i=1}^{m}{\\lambda_{i}}}{\\sum_{j=1}^{M}{\\lambda_{j}}}, $$\n",
    "where $\\lambda$ are eigenvalues, $M$ is the dimensionality of the original data, and $m$ is the chosen lower dimensionality. \n",
    "\n",
    "Using the $POV$ formula we may select a number $m$ of dimensions/eigenvalues so that the proportion of variance is, for instance, equal to 95%.\n",
    "\n",
    "Implement a new PCA for encoding and decoding that receives in input not the number of dimensions for projection, but the amount of proportion of variance to be preserved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_decode_PCA_with_pov(A,p):\n",
    "    # INPUT:\n",
    "    # A    [NxM] numpy data matrix (N samples, M features)\n",
    "    # p    float number between 0 and 1 denoting the POV to be preserved\n",
    "    #\n",
    "    # OUTPUT:\n",
    "    # Ahat [NxM] numpy PCA reconstructed data matrix (N samples, M features)\n",
    "    # m    integer reporting the number of dimensions selected\n",
    "    \n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the *lfw* dataset using the *get_lfw_data()* in *syntheticdata*. Use the implemented function to encode and decode the data by projecting on a lower dimensional space such that POV=0.9. Use the function *imshow* to plot and compare original and reconstructed pictures. Comment on the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X,y,h,w = syntheticdata.get_lfw_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xhat,m = encode_decode_PCA_with_pov(X,None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(X[0,:].reshape((h, w)), cmap=plt.cm.gray)\n",
    "plt.figure()\n",
    "plt.imshow(Xhat[0,:].reshape((h, w)), cmap=plt.cm.gray)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Comment:** enter your comment here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optional 2: Quantitative Assessment of K-Means\n",
    "\n",
    "We used k-means for clustering and we assessed the results qualitatively by visualizing them. However we often want to be able to measure in a quantitative way how good the clustering was. To do this, we will use a classification task to evaluete numerically the goodness of the representation learned via k-means.\n",
    "\n",
    "Reload the *iris* dataset. Import a standard *RidgeClassifier* from the module *sklearn.linear_model*. Use the k-means representations learned previously (*yhat2,...,yhat5*) and the true label to train the classifier. Evaluate your model on the training data (we do not have a test set, so this procedure will assess the model fit instead of generalization) using the *accuracy_score()* function from the *sklearn.metrics* module. Plot a graph showing how the accuracy score varies when changing the value of k. Comment on the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X,y = syntheticdata.get_iris_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import RidgeClassifier\n",
    "from sklearn import metrics\n",
    "lr = RidgeClassifier()\n",
    "\n",
    "lr.fit(None)\n",
    "yhat = lr.predict(None)\n",
    "print(metrics.accuracy_score(y,yhat))\n",
    "\n",
    "lr.fit(yhat2.reshape([yhat2.size,1]),y)\n",
    "yhat = lr.predict(yhat2.reshape([yhat2.size,1]))\n",
    "acc2 = metrics.accuracy_score(y,yhat)\n",
    "\n",
    "None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Comment:** enter your comment here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusions \n",
    "\n",
    "In this notebook we studied **unsupervised learning** considering two important and representative algorithms: **PCA** and **k-means**.\n",
    "\n",
    "First, we implemented the PCA algorithm step by step; we then run the algorithm on synthetic data in order to see its inner workings and evaluate when it may make sense to use it and when not. We then considered two typical uses of PCA: for **visualization** on the *iris* dataset, and for **compression-decompression** on the *lfw* dataset.\n",
    "\n",
    "We then moved to consider the k-means algorithm. In this case we used the implementation provided by *scikit-learn* and we applied it to another prototypical unsupervised learning problem: **clustering**; we used *k-means* to process the *iris* dataset and we evaluated the results visually.\n",
    "\n",
    "In the final optional part, we considered two additional questions that may arise when using the above algorithms. For PCA, we considered the problem of **selection of hyper-parameters**, that is, how we can select the hyper-parameter of our algorithm in a reasonable fashion. For k-means, we considered the problem of the **quantitative evaluation** of our results, that is, how can we measure the performance or usefulness of our algorithms. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
